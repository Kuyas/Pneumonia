{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "\n",
    "EPOCHS_ARRAY = [1,5,10,15,30]\n",
    "BATCH_SIZE_ARRAY = [1,2,4,8,10,16,24]\n",
    "TRAIN_SIZE_ARRAY = [200,400,600,800,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(epochs_arr,batchsize_arr,train_size_arr):\n",
    "    EPOCHS = epochs_arr\n",
    "    TRAIN_BATCH_SIZE = batchsize_arr\n",
    "    TRAIN_SIZE = train_size_arr\n",
    "\n",
    "    BATCH_SIZE = 10\n",
    "\n",
    "    LEARNING_RATE = 0.0003\n",
    "    TRAIN_DATAPATH = 'chest_xray\\\\train'\n",
    "    VAL_DATAPATH = 'chest_xray\\\\val'\n",
    "    TEST_DATAPATH = 'chest_xray\\\\test'\n",
    "    TRAIN_TRANSFORM_IMG = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees = 15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    VAL_TRANSFORM_IMG = transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATAPATH, transform=TRAIN_TRANSFORM_IMG)\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True,num_workers=4)\n",
    "    val_data = torchvision.datasets.ImageFolder(root=TEST_DATAPATH, transform=VAL_TRANSFORM_IMG)\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n",
    "    test_data = torchvision.datasets.ImageFolder(root=TEST_DATAPATH, transform=VAL_TRANSFORM_IMG)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n",
    "\n",
    "    print_row = []\n",
    "    # def imshow(img):\n",
    "    #     img = img / 2 + 0.5     # unnormalize\n",
    "    #     npimg = img.numpy()\n",
    "    #     plt.figure(figsize=(20,20))\n",
    "    #     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "    # dataiter = iter(train_data_loader)\n",
    "    # images, labels = dataiter.next()\n",
    "    # # show images\n",
    "    # imshow(torchvision.utils.make_grid(images))\n",
    "#     print(\"Number of Training Examples: \", len(train_data))\n",
    "#     print(\"Number of Test Examples: \", len(test_data))\n",
    "#     print(\"Number of Valid Examples: \", len(val_data))\n",
    "#     print(\"Detected Classes are: \", train_data.class_to_idx)\n",
    "    train_iter = iter(train_data_loader)\n",
    "    images, labels_ = train_iter.next()\n",
    "#     print(\"Image Shape on Batch size = {} \".format(images.size()))\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     print(device)\n",
    "\n",
    "    model = models.vgg19(True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.classifier[6] = nn.Sequential(nn.Linear(4096, 256), \n",
    "                          nn.ReLU(), \n",
    "                          nn.Dropout(0.4),\n",
    "                          nn.Linear(256, 2),                   \n",
    "                          nn.LogSoftmax(dim=1))\n",
    "\n",
    "#     print(model.classifier)\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    _params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adam(_params, lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        correct = 0 \n",
    "        iterations = 0\n",
    "        iter_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i,data in enumerate(train_data_loader,0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs,labels)\n",
    "            iter_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum()\n",
    "            iterations += 1\n",
    "            if(iterations == TRAIN_SIZE):\n",
    "                break\n",
    "\n",
    "        train_loss.append(iter_loss/iterations)\n",
    "        # Record the training accuracy\n",
    "        train_accuracy.append((100 * correct / len(train_data)))\n",
    "\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "        model.eval()\n",
    "\n",
    "        for i, data in enumerate(val_data_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)     \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "            iterations+=1\n",
    "\n",
    "        test_loss.append(loss/iterations)\n",
    "        # Record the Testing accuracy\n",
    "        test_accuracy.append((100 * correct / len(val_data)))\n",
    "        stop = time.time()\n",
    "#         print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Validation Loss: {:.3f}, Validation Acc: {:.3f}, Time: {:.3f}s'\n",
    "#                .format(epoch+1, EPOCHS, train_loss[-1], train_accuracy[-1], test_loss[-1], test_accuracy[-1], stop-start))\n",
    "\n",
    "        row = [epoch+1,train_loss[-1],train_accuracy[-1].item(),test_loss[-1].item(),test_accuracy[-1].item(),stop-start]\n",
    "        print_row.append(row)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    classes =['NORMAL','PNEUMONIA']\n",
    "    confusion_matrix = torch.zeros(2, 2)\n",
    "    correct_0 = 0\n",
    "    correct_1 = 0\n",
    "    class_correct = list(0. for i in range(2))\n",
    "    class_total = list(0. for i in range(2))\n",
    "    with torch.no_grad():\n",
    "        for data in test_data_loader:\n",
    "            images,labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "            correct+=(predicted == labels).sum().item()\n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "    acc = 100*correct/total\n",
    "#     print(correct)\n",
    "#     print(total)\n",
    "#     print('Testing accuracy: ' + str(acc))\n",
    "#     print(confusion_matrix)\n",
    "\n",
    "#     for i in range(2):\n",
    "#         print('Accuracy of %5s : %2d %%' % (\n",
    "#             classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    acc0 = 100 * class_correct[0] / class_total[0]\n",
    "    acc1 = 100 * class_correct[1] / class_total[1]\n",
    "    print_row_end = []\n",
    "    row_end = [acc,acc0,acc1]\n",
    "    print_row_end.append(row_end)\n",
    "\n",
    "    filename = \"vgg19+base_\"+str(TRAIN_BATCH_SIZE)+\"_\"+str(TRAIN_SIZE)+\"_\"+str(EPOCHS)\n",
    "    heading = []\n",
    "    footer = []\n",
    "    heading.append([\"Epoch Number\", 'Training Loss', 'Training Accuracy','Validation Loss','Validation Acc', 'Time' ])\n",
    "    footer.append(['Testing Accuracy','Accuracy of Normal', 'Accuracy of Pneumonia'])\n",
    "    with open('New/'+filename+'.csv', mode='w',newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerows(heading)\n",
    "        writer.writerows(print_row)\n",
    "        writer.writerows(footer)\n",
    "        writer.writerows(print_row_end)\n",
    "    import winsound\n",
    "    frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "    duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "    winsound.Beep(frequency, duration)\n",
    "\n",
    "#     print(\"PyTorch version: \", torch.__version__ )\n",
    "#     print(\"CUDA available: \", torch.cuda.is_available())\n",
    "#     print(\"CUDA version: \", torch.version.cuda)\n",
    "    print(filename)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg19+base_1_200_1\n",
      "vgg19+base_1_400_1\n",
      "vgg19+base_1_600_1\n",
      "vgg19+base_1_800_1\n",
      "vgg19+base_1_-1_1\n",
      "vgg19+base_2_200_1\n",
      "vgg19+base_2_400_1\n",
      "vgg19+base_2_600_1\n",
      "vgg19+base_2_800_1\n",
      "vgg19+base_2_-1_1\n",
      "vgg19+base_4_200_1\n",
      "vgg19+base_4_400_1\n",
      "vgg19+base_4_600_1\n",
      "vgg19+base_4_800_1\n",
      "vgg19+base_4_-1_1\n",
      "vgg19+base_8_200_1\n",
      "vgg19+base_8_400_1\n",
      "vgg19+base_8_600_1\n",
      "vgg19+base_8_800_1\n",
      "vgg19+base_8_-1_1\n",
      "vgg19+base_10_200_1\n",
      "vgg19+base_10_400_1\n",
      "vgg19+base_10_600_1\n",
      "vgg19+base_10_800_1\n",
      "vgg19+base_10_-1_1\n",
      "vgg19+base_16_200_1\n",
      "vgg19+base_16_400_1\n",
      "vgg19+base_16_600_1\n",
      "vgg19+base_16_800_1\n",
      "vgg19+base_16_-1_1\n",
      "vgg19+base_24_200_1\n",
      "vgg19+base_24_400_1\n",
      "vgg19+base_24_600_1\n",
      "vgg19+base_24_800_1\n",
      "vgg19+base_24_-1_1\n",
      "vgg19+base_1_200_5\n",
      "vgg19+base_1_400_5\n",
      "vgg19+base_1_600_5\n",
      "vgg19+base_1_800_5\n",
      "vgg19+base_1_-1_5\n",
      "vgg19+base_2_200_5\n",
      "vgg19+base_2_400_5\n",
      "vgg19+base_2_600_5\n",
      "vgg19+base_2_800_5\n",
      "vgg19+base_2_-1_5\n",
      "vgg19+base_4_200_5\n",
      "vgg19+base_4_400_5\n",
      "vgg19+base_4_600_5\n",
      "vgg19+base_4_800_5\n",
      "vgg19+base_4_-1_5\n",
      "vgg19+base_8_200_5\n",
      "vgg19+base_8_400_5\n",
      "vgg19+base_8_600_5\n",
      "vgg19+base_8_800_5\n",
      "vgg19+base_8_-1_5\n",
      "vgg19+base_10_200_5\n",
      "vgg19+base_10_400_5\n",
      "vgg19+base_10_600_5\n",
      "vgg19+base_10_800_5\n",
      "vgg19+base_10_-1_5\n",
      "vgg19+base_16_200_5\n",
      "vgg19+base_16_400_5\n",
      "vgg19+base_16_600_5\n",
      "vgg19+base_16_800_5\n",
      "vgg19+base_16_-1_5\n",
      "vgg19+base_24_200_5\n",
      "vgg19+base_24_400_5\n",
      "vgg19+base_24_600_5\n",
      "vgg19+base_24_800_5\n",
      "vgg19+base_24_-1_5\n",
      "vgg19+base_1_200_10\n",
      "vgg19+base_1_400_10\n",
      "vgg19+base_1_600_10\n",
      "vgg19+base_1_800_10\n",
      "vgg19+base_1_-1_10\n",
      "vgg19+base_2_200_10\n",
      "vgg19+base_2_400_10\n",
      "vgg19+base_2_600_10\n",
      "vgg19+base_2_800_10\n",
      "vgg19+base_2_-1_10\n",
      "vgg19+base_4_200_10\n",
      "vgg19+base_4_400_10\n",
      "vgg19+base_4_600_10\n",
      "vgg19+base_4_800_10\n",
      "vgg19+base_4_-1_10\n",
      "vgg19+base_8_200_10\n",
      "vgg19+base_8_400_10\n",
      "vgg19+base_8_600_10\n",
      "vgg19+base_8_800_10\n",
      "vgg19+base_8_-1_10\n",
      "vgg19+base_10_200_10\n",
      "vgg19+base_10_400_10\n",
      "vgg19+base_10_600_10\n",
      "vgg19+base_10_800_10\n",
      "vgg19+base_10_-1_10\n",
      "vgg19+base_16_200_10\n",
      "vgg19+base_16_400_10\n",
      "vgg19+base_16_600_10\n",
      "vgg19+base_16_800_10\n"
     ]
    }
   ],
   "source": [
    "for i in EPOCHS_ARRAY:\n",
    "    for j in BATCH_SIZE_ARRAY:\n",
    "        for k in TRAIN_SIZE_ARRAY:\n",
    "            run(i,j,k)\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
